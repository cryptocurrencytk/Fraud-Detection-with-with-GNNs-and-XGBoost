# -*- coding: utf-8 -*-
"""Fraud Detection with with GNNs and XGBoost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jGAWyO86cHTo3tnGbKCmXS5WoIc8X2jO
"""

pip install torch torchvision torchaudio torch-geometric networkx scikit-learn xgboost

import torch
import torch.nn.functional as F
import pandas as pd
import networkx as nx
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, GraphSAGE
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc
import xgboost as xgb
from imblearn.over_sampling import SMOTE, ADASYN
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/fraud_credit_card_transactions.csv")

# Preprocessing
# df['Amount'] = df['Amount'].replace('[\$,]', '', regex=True).astype(float)
# df[['Hour', 'Minute']] = df['Time'].str.split(':', expand=True).astype(int)
# df.drop(columns=['Time', 'Errors?'], inplace=True)
# df['Merchant State'].fillna('Unknown', inplace=True)
# df['Zip'].fillna(df['Zip'].mode()[0], inplace=True)
# df['Is Fraud?'] = df['Is Fraud?'].map({'Yes': 1, 'No': 0})
df['Amount'] = df['Amount'].replace('[\$,]', '', regex=True).astype(float)
df[['Hour', 'Minute']] = df['Time'].str.split(':', expand=True).astype(int)
df.drop(columns=['Time', 'Errors?'], inplace=True)

# Replace missing values in 'Merchant State' and 'Zip'
df['Merchant State'] = df['Merchant State'].fillna('Unknown')
df['Zip'] = df['Zip'].fillna(df['Zip'].mode()[0])

# Map 'Is Fraud?' column to binary values
df['Is Fraud?'] = df['Is Fraud?'].map({'Yes': 1, 'No': 0})

# Construct Graph and map nodes to continuous indices
G = nx.Graph()
node_type_mapping = {}  # To store mapping of node to index
node_idx = 0

for node_type in ['User', 'Card', 'Merchant Name']:
    for node in df[node_type].unique():
        if node not in node_type_mapping:
            node_type_mapping[node] = node_idx
            G.add_node(node_idx, type=node_type)  # Add node with index
            node_idx += 1

# Add edges using mapped indices
edges = []
for _, row in df.iterrows():
    user_idx = node_type_mapping[row['User']]
    card_idx = node_type_mapping[row['Card']]
    merchant_idx = node_type_mapping[row['Merchant Name']]
    edges.extend([[user_idx, card_idx], [card_idx, merchant_idx]])

# Convert edges to tensor format for PyG
edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()

# Node Features (Using Amount, Hour, Minute)
# You might want to create features based on your data here
# For now, using random features
node_features = torch.randn((len(G.nodes), 16))

# Labels (Fraud or Not)
labels = torch.zeros(len(G.nodes))  # Default to 0 (Not Fraud)
for _, row in df.iterrows():
    if row['Is Fraud?'] == 1:
        user_idx = node_type_mapping[row['User']]
        labels[user_idx] = 1  # Mark fraud users

# Create PyG Graph Data
graph_data = Data(x=node_features, edge_index=edge_index, y=labels)

# Define a Simple GNN Model (GraphSAGE)
class GNN(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GNN, self).__init__()
        self.conv1 = GraphSAGE(input_dim, hidden_dim, num_layers=1)
        self.conv2 = GraphSAGE(hidden_dim, output_dim, num_layers=1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Train GNN
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = GNN(input_dim=16, hidden_dim=32, output_dim=64).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
loss_fn = torch.nn.CrossEntropyLoss()

graph_data = graph_data.to(device)

for epoch in range(100):
    model.train()
    optimizer.zero_grad()
    out = model(graph_data.x, graph_data.edge_index)
    loss = loss_fn(out, graph_data.y.long())
    loss.backward()
    optimizer.step()
    if epoch % 10 == 0:
        print(f"Epoch {epoch} | Loss: {loss.item()}")

# Get Node Embeddings from GNN
model.eval()
with torch.no_grad():
    embeddings = model(graph_data.x, graph_data.edge_index).cpu().numpy()

# Create a DataFrame for embeddings
embedding_df = pd.DataFrame(embeddings, index=list(G.nodes), columns=[f'embed_{i}' for i in range(64)])

# Merge embeddings with fraud labels
# Use node_type_mapping to merge correctly
df_merged = df[['User', 'Card', 'Merchant Name', 'Is Fraud?']].copy()

# Merge using node_type_mapping to get correct indices
df_merged['User_idx'] = df_merged['User'].map(node_type_mapping)
df_merged = df_merged.merge(embedding_df, left_on='User_idx', right_index=True, how='left')

# Similar merges for Card and Merchant Name
df_merged['Card_idx'] = df_merged['Card'].map(node_type_mapping)
df_merged = df_merged.merge(embedding_df, left_on='Card_idx', right_index=True, how='left', suffixes=('_user', '_card'))

df_merged['Merchant Name_idx'] = df_merged['Merchant Name'].map(node_type_mapping)
df_merged = df_merged.merge(embedding_df, left_on='Merchant Name_idx', right_index=True, how='left', suffixes=('_user', '_merchant'))

df_merged.drop(columns=['User_idx', 'Card_idx', 'Merchant Name_idx'], inplace=True)  # Remove temporary index columns

# Drop NaN values from missing embeddings
df_merged.dropna(inplace=True)

# Prepare dataset for training
X = df_merged.drop(columns=['User', 'Card', 'Merchant Name', 'Is Fraud?'])
y = df_merged['Is Fraud?']

# Feature Scaling (Standardization)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data for training and testing
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=888)

# --- SMOTE & ADASYN for Imbalanced Data ---
# Experiment with SMOTE and ADASYN for oversampling the minority class
# smote = SMOTE(random_state=888)
# X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Alternatively, you can try ADASYN:
adasyn = ADASYN(random_state=42)
X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)

# --- XGBoost Model ---
# Hyperparameter tuning for XGBoost
xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=888,
    scale_pos_weight=5  # Adjust for imbalanced data
)

# Hyperparameter grid search
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.05, 0.1, 0.2],
    'scale_pos_weight': [2, 3, 5, 10]  # Experiment with different weight ratios
}

grid_search = GridSearchCV(xgb_model, param_grid, scoring='accuracy', cv=3)
grid_search.fit(X_train_resampled, y_train_resampled)

# Best parameters found through grid search
print("Best parameters:", grid_search.best_params_)

# Train XGBoost model with best parameters
best_xgb_model = grid_search.best_estimator_

# --- Model Evaluation ---
y_pred = best_xgb_model.predict(X_test)
y_pred_proba = best_xgb_model.predict_proba(X_test)[:, 1]

# ROC Curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

# --- Adjust Decision Threshold ---
threshold = 0.3  # Adjust threshold for better sensitivity to fraud
y_pred_adjusted = (y_pred_proba > threshold).astype(int)

print("Adjusted Accuracy:", accuracy_score(y_test, y_pred_adjusted))
print("Adjusted Classification Report:\n", classification_report(y_test, y_pred_adjusted))